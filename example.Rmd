---
title: "Process CTD data using gapctd"
author: "Sean Rohan"
output: html_document
---

*WARNING: Knitting this document will not process the data. Please run code blocks one at a time.*

## Introduction

This document demonstrates how to use the gapctd package to process CTD data collected on RACE Groundfish Surveys and generate data products. This process was developed for use with SBE19plus V2 CTDs with induction pumps that are deployed on bottom-trawl survey gear in the eastern Bering Sea, Gulf of Alaska, and Aleutian Islands. Use of this package requires a connection to the AFSC RACEBASE and RACE_DATA data tables or comma separated value tables provided by AFSC staff.

## 1. Installation

This package requires an installation of SBE Data Processing software (available through manufacturer) and the gapctd package. The latest version of the gapctd package can be installed from GitHub as follows:

```{r install, include=TRUE, eval=FALSE}
devtools::install_github("sean-rohan-noaa/gapctd")
```


## 2. Load package, define global variables, and connect to Oracale

Load the gapctd package. Assign vessel, year, and region (BS, GOA, or AI) variables and establish a connection to Oracle using the RODBC package.

```{r connect, include=TRUE, eval=FALSE}
library(gapctd)

# Select vessel, year, and region
vessel <- 162
year <- 2021
region <- "BS"

# Establish Oracle connection connection
channel <- gapctd::get_connected(schema = "AFSC")
```

## 3. Setup processing directory

The gapctd package processes data within a directory that contains binary CTD data files (.hex), configuration files (.xmlcon), batch files (.bat) that provide instructions for running modules in SBE Data Processing, and Program Setup Files (.psa) that specify the parameters for batch processing modules. The package produces converted CTD data files (.cnv) and CSV files that contain summary haul data. To set up the directory, run the setup_ctd_processing_dir function in your working directory. Arguments that must be provided to the setup_ctd_processing_dir function are the directory that contains binary CTD data files (.hex) and configuration files (.xmlcon) and the type of CTD unit (i.e, "sbe19plus") that was used to collect the data.

The function does the 

<ol type = "1">
<li>Setting up a directory structure used for data processing.</li>
<li>Copying hexadecimal data (.hex) files to subdirectory 'data'.</li>
<li>Copying configuration (.xmlcon) files from the user-specified directory (e.g., "G:/RACE_CTD/data/2021/ebs/v162") to subdirectory 'psa_xmlcon'.</li>
<li>Copying batch (.bat) processing routine files for the selected CTD model (e.g., ctd_unit = "sbe19plus") that are included in the gapctd package to the root of your working directory.</li>
<li>Copying program setup files (.psa) for the specified CTD model to the subdirectory 'psa_xmlcon'.</li>
</ol>

```{r setup, include=TRUE, eval=FALSE}
# Move files from storage to local directory
gapctd::setup_ctd_processing_dir(ctd_dir = "G:/RACE_CTD/data/2021/ebs/v162",
                                 ctd_unit = "sbe19plus",
                                 recursive = TRUE) # TRUE ensures files are copied from nested subdirectories
```

## 4. Batch process hex files

Modules in the SBE Data Processing software are used to convert .hex data files to a human-readable format, apply filters and corrections to the data, and derive physical variables (e.g., Practical Salinity (PSS-78), density) from filtered and corrected instrument measurements. The run_sbe_batch function runs batch (.bat) files in command line to run SBE modules on data from each haul, retrieves haul metadata from RACEBASE, matches CTD data to georeferenced haul metadata, and creates NMEA files that are used for deriving TEOS-10 variables using SBE Data Processing (using gapctd::create_NMEA_files).



```{r batch, include=TRUE, eval=FALSE}
gapctd::run_sbe_batch(vessel = vessel,
                      year = year,
                      region = region,
                      rodbc_channel = channel)
```


## 5. Retrieve on-bottom and off-bottom times

GAP's CTD data product includes averages of on-bottom physical variables (e.g., PSS-78, temperature) for each haul while the trawl was on-bottom. To calculate these on-bottom averages, it is necessary to subset the on-bottom scans from each deployment. The get_haul_events function retrieves on-bottom, marked event, haul back, and off-bottom events from RACE_DATA tables and joins them with haul metadata from the survey_metadata_[year].csv file. 

```{r events, include=TRUE, eval=FALSE}
gapctd::get_haul_events(channel = channel,
                        append_haul_metadata = TRUE)
```

## 6. Calculate mean bottom temperature and salinity from each deployment

The calc_bottom_mean function uses on-bottom and off-bottom times to calculate average bottom temperature and salinity from each deployment's file with derived quantities (e.g., pattern = "TEOS10.cnv" for cnv files with derived TEOS-10 variables). A time buffer in seconds (e.g., time_buffer = 30) is added to each on-bottom time and off-bottom time. Mean bottom temperature and salinity are written to the haul metadata csv file when argument append_haul_metadata is set to TRUE. 

```{r avg_bottom, include=TRUE, eval=FALSE}
gapctd::calc_bottom_mean(haul_metadata_path = list.files(paste0(getwd(), "/metadata/"), 
                                                         full.names = TRUE),
                             pattern = "TEOS10.cnv",
                             timezone = "America/Anchorage",
                             time_buffer = 30,
                             append_haul_metadata = TRUE)
```

## 7. Move bad station data to bad_cnv directory

In some cases, the CTD was turned on at the surface without being deployed or was shut off during the middle of a downcast during a deployment. The remove_bad_station_data function facilitates identification of such files and moves all cnv files from such files to the bad_cnv subdirectory. Arguments in the remove_bad_station_data function specify:

<ul>
<li><i>max_diff_bt_ctd_depth</i>: The maximum allowable difference between RACEBASE.GEAR_DEPTH while and mean CTD depth while on-bottom. This criteria handles cases where the CTD stopped collecting data during the downcast.</li>
<li><i>min_ctd_depth</i>: The minimum depth for a deployment to be valid. This criteria handles cases where the CTD was turned on at the surface.</li>
</ul>

```{r remove_bad_stn, include=TRUE, eval=FALSE}
gapctd::remove_bad_station_data(vessel = vessel,
                                year = year,
                                region = region,
                                haul_metadata_path = list.files(paste0(getwd(), "/metadata/"), 
                                                                full.names = TRUE),
                                max_diff_bt_ctd_depth = 10,
                                min_ctd_depth = 5)
```

## 8. Manually flag and remove bad data 

Although batch processing fixes some sampling artifacts, other problems tend to persist. In the manual point flagging step, 'bad' data are flagged, removed, and estimated via interpolation from the remaining data. Users can either specify which files to review using the 'file_paths' argument or retrieve all downcasts and upcasts and downcasts from a directory. Interpolation is performed using the method specified by the user (e.g., interpolation_method = "unesco"; see function documentation for details).


```{r manual_flag, include=TRUE, eval=FALSE}
# NOTE: This code block cannot be run as a chunk in an Rmd file because base graphic interfaces do not work within an R Markdown document.
gapctd::manual_flag_interpolate(file_paths = NULL,
                                haul_metadata_path = list.files(here::here("metadata"), 
                                                                full.names = TRUE),
                                cast_dir_filepath = here::here("data", "tm_correct"),
                                year = year,
                                var = c("salinity", "temperature"),
                                z_var = "pressure",
                                flag_filename = here::here("output", "manual_flag_points.csv"),
                                start_index = NULL,
                                interpolation_method = "unesco",
                                var_range_min = 0.1)
```

Bad data are flagged using base R's graphical user interface (GUI) plot() and identify() functions. 

To flag and remove points on the plots:

<ol type = "1">
  <li> Click on each point that should be flagged for removal.</li>
  <li>Press 'Esc' when points have been flagged</li>
  <li>Enter a response to the prompt in the console indicating whether to accept the profile or remove more points ("Accept profile (y) or remove additional points (n)?:"). *RECOMMENDATION: If any points were flagged for removal, select the 'remove additional points (n)' option an additional time to the regenerate the plot without the flagged points to verify that additional points do not need to be flagged.*.</li>
  <li>Repeat #1-3 for every profile and variable.</li>
</ol>

The manual_flag_interpolate() function searches the output directory for a flagged date file prior to opening the GUI for a cast and *does not* overwrite data. Therefore, the function can be stopped and will restart at the next file.  

## 9. Review flagged and interpolated profiles

After manually reconciling sampling artifacts in the data, profiles are visually inspected on a haul-by-haul basis using the manual_flag_review() function to assess whether there are any remaining problems with the data. Hauls with remaining problems are removed and can be reprocessed manually using manual_flag_interpolate(), i.e., step #8.

```{r manual_review, include=TRUE, eval=FALSE}


```



## 10. Calculate surface temperature and salinity from each upcast

The calc_surface_mean function calculates mean temperature and salinity from upcast files in the final_cnv subdirectory. Mean surface temperature and salinity are written to the haul metadata csv file when argument append_haul_metadata is set to TRUE.

```{r avg_surface, include=TRUE, eval=FALSE}
gapctd::calc_surface_mean(haul_metadata_path = list.files(paste0(getwd(), "/metadata/"), 
                                                          full.names = TRUE),
                              depth_interval = c(1,2),
                              append_haul_metadata = TRUE)
```


## Make plots

```{r plot, include=TRUE, eval=FALSE}
gapctd::make_cast_plots()
```

## Disclaimer 

Reference in this document to trade names does not imply endorsement by the National Marine Fisheries Service, NOAA.

## References
